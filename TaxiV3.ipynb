{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf7W39xQ6aThUgFn3Dr5hV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shmanth/Taxi_v3_open_ai_gym_solution_Q_learning_sarsa/blob/master/TaxiV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving OpenAI Gym Environment - (Taxi-*v3*)\n",
        "In this Python demo, we'll try solving the classic cab-driver problem. The purpose of this notebook is to show how to solve OpenAI Gym environments. We'll demonstrate Q-learning & SARSA on the Taxi environment.\n",
        "\n",
        "Let's now look at the problem statement\n",
        "\n",
        "Here, the objective is to pick up the passenger from one position and drop them off at another in minimum possible time. For this problem, we'll consider our environment to be a 5x5 grid.\n",
        "\n",
        " Image source: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "There are 4 locations (R, G, Y, B) marked in the image. And the task is to pick up the passenger from one of the four locations and drop him off at other. There is a reward of +20 for a successful dropoff, and -1 for every timestep it takes and -10 for illegal pick-up and drop-off actions."
      ],
      "metadata": {
        "id": "Sutq5Bsm0_6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xt3O9GYJxeOn"
      },
      "outputs": [],
      "source": [
        "# Import routines\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling Env"
      ],
      "metadata": {
        "id": "MqMDeX0u1TN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\")# Create the env\n",
        "\n",
        "state = env.reset()\n",
        "env.render() # Helps in visualizing the environment\n",
        "\n",
        "print(\"Current state is:\", state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO01BQUg1FdT",
        "outputId": "4ad85410-1863-471d-9eb5-03b43aa8dd1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state is: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rendering:"
      ],
      "metadata": {
        "id": "SJ9qowT9ILII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- yellow: taxi is unoccupied\n",
        "- green: taxi is occupied by a passenger\n",
        "- blue: passenger\n",
        "- magenta: destination\n",
        "- other grids: locations"
      ],
      "metadata": {
        "id": "FLbxsh_DjRPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### State Space\n",
        "The state vector for this problem is (col_index, row_index, destination_locations, passenger_position) There are 5 rows, 5 columns and 4 destination locations. What about the passenger locations? 4 or 5?\n",
        "\n",
        "If the passenger is not in cab that means he could be only at one of the four locations. But we also need to account for 1 addition state if the passenger is inside the cab. So, passenger could be at any 4+1 possible locations.\n",
        "\n",
        "Therefore, the state space = 5x5x4x5 = 500"
      ],
      "metadata": {
        "id": "9TDY9IVpizGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No. of possible states\n",
        "state_size= env.observation_space.n\n",
        "print(\"state_space: \", state_size)"
      ],
      "metadata": {
        "id": "IhvL2bzO1Fow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b470aeb1-db24-4fb3-b8b9-695363376269"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_space:  500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action Space\n",
        "At any state, the cab driver can either move in any of the four directions or it can pickup/ drop (legally or illegally)\n",
        "\n",
        "- 0: south\n",
        "- 1: north\n",
        "- 2: east\n",
        "- 3: west\n",
        "- 4: pickup\n",
        "- 5: drop"
      ],
      "metadata": {
        "id": "edh4nxElo61y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No. of possible Actions\n",
        "action_size= env.action_space.n\n",
        "print(\"Action_space: \", action_size)"
      ],
      "metadata": {
        "id": "mSNQbqs81Fq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bab0c19-1b65-4334-f631-acf0275238c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action_space:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Let's now solve the given MDP using Q-learning & SARSA."
      ],
      "metadata": {
        "id": "3rCEJDqEpdaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning\n",
        "Q-Learning is an off-policy optimal control algorithm. It learns the Q-values by taking the next action based on the greedy policy"
      ],
      "metadata": {
        "id": "4nZQXZnapjVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_table = np.zeros((state_size, action_size))\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvG-2a7uiMUx",
        "outputId": "8c64349b-9552-49d0-d8b1-46a978b622a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 100000    # No. of episodes\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.1 # Learning rate\n",
        "gamma = 0.8         # Discount factor\n",
        "epsilon = 0.1       # Exploration- exploitation Tradeoff"
      ],
      "metadata": {
        "id": "zLehLTwViMXG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping the policy epsilon-greedy\n",
        "def epsilon_greedy(state, table):\n",
        "  z = np.random.random()\n",
        "  if z > epsilon:\n",
        "    action = np.argmax(table[state])\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "  return action"
      ],
      "metadata": {
        "id": "xdmbR_nysqDF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "deltas= []\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  step = 0\n",
        "  biggest_change = 0\n",
        "\n",
        "  if episode % 5000 == 0:\n",
        "    print(\"Episode: {}\".format(episode))\n",
        "\n",
        "  while not done:\n",
        "    action = epsilon_greedy(state, Q_table)\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    oldQ_table = Q_table[state, action]\n",
        "    Q_table[state, action] += learning_rate * (reward + gamma * np.max(Q_table[new_state,:])-Q_table[state, action])\n",
        "    biggest_change = max(biggest_change, np.abs(Q_table[state][action]-oldQ_table))\n",
        "    state = new_state\n",
        "  deltas.append(biggest_change)\n",
        "  if deltas[-1] < 0.00000001:\n",
        "    break\n",
        "  episode +=1\n",
        "\n",
        "end= time.time()\n",
        "training_time = end-start\n",
        "print(\"Time_taken in seconds: \", training_time)\n",
        "print(\"maximum difference\", deltas[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nppFzA-ksqFu",
        "outputId": "481b5320-9752-4f16-84a1-ca9123f8f756"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time_taken in seconds:  7.379796981811523\n",
            "maximum difference 9.834179692802536e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_table[33]"
      ],
      "metadata": {
        "id": "XHrbFkg3sqIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87cdf2e4-1b5d-4f52-fd29-e37e55732fc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.60185896, -3.72711837, -3.70711057, -3.71124799, -6.62370001,\n",
              "       -4.76883803])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Q-Table\n",
        "Let's know test our Q-learning agent on a different environment"
      ],
      "metadata": {
        "id": "xpkEGdYYDQ8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "roLgq1rmsqLl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "\n",
        "while (done==False):\n",
        "  best_action = np.argmax(Q_table[state,:])\n",
        "  state,reward, done, info = env.step(best_action)\n",
        "  cumulative_reward += reward\n",
        "\n",
        "  time.sleep(0.5)\n",
        "  clear_output(wait=True)\n",
        "  env.render()\n",
        "  print(\"episode_reward\", cumulative_reward)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5iTnF2tUsqPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7650054a-4559-4738-9c4c-34288fda09bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode_reward 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA\n",
        "SARSA is on-policy learning algorithm. Unlinke Q-learning, it learns the Q-values by taking the next action based on the current policy rather than a greedy policy"
      ],
      "metadata": {
        "id": "k8OYPdT0VhZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state= env.reset()\n",
        "env.render()\n",
        "\n",
        "print(\"Current state is:\", state)"
      ],
      "metadata": {
        "id": "oPgu7_k9sqTf",
        "outputId": "95839571-e187-412c-8ec2-4789e2213b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state is: 361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa_table = np.zeros((state_size, action_size))\n",
        "\n",
        "print(sarsa_table)"
      ],
      "metadata": {
        "id": "0x0m3nj4iMcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4983129-45d6-4819-94e4-03cedc9ee2ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 800000\n",
        "start = time.time()\n",
        "deltas= []\n",
        "\n",
        "for episode in range(1,episodes+1):\n",
        "\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  biggest_change = 0\n",
        "\n",
        "  if episode % 10000 == 0:\n",
        "    print(\"Current episode is:{}\".format(episode))\n",
        "  while not done:\n",
        "    action = epsilon_greedy(state, sarsa_table)\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    old_sarsa_table = sarsa_table[state, action]\n",
        "\n",
        "    sarsa_table[state, action] += learning_rate * (reward + gamma*sarsa_table[next_state, action]-sarsa_table[state, action])\n",
        "\n",
        "    biggest_change = max(biggest_change, np.abs(sarsa_table[state][action] - old_sarsa_table))\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    if deltas[-1] < 0.00000001:\n",
        "      break\n",
        "\n",
        "    episode += 1\n",
        "\n",
        "end = time.time()\n",
        "training_time= start - end\n",
        "print(\"Total_time_taken is: \",training_time)\n",
        "print(\"Maximum difference\", deltas[-1])"
      ],
      "metadata": {
        "id": "N9jUWi3HiMfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c439d3af-da73-4604-9ad0-2aaf9b4831a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current episode is:10000\n",
            "Current episode is:20000\n",
            "Current episode is:30000\n",
            "Current episode is:40000\n",
            "Current episode is:50000\n",
            "Current episode is:60000\n",
            "Current episode is:70000\n",
            "Current episode is:80000\n",
            "Current episode is:90000\n",
            "Current episode is:100000\n",
            "Current episode is:110000\n",
            "Current episode is:120000\n",
            "Current episode is:130000\n",
            "Current episode is:140000\n",
            "Current episode is:150000\n",
            "Current episode is:160000\n",
            "Current episode is:170000\n",
            "Current episode is:180000\n",
            "Current episode is:190000\n",
            "Current episode is:200000\n",
            "Current episode is:210000\n",
            "Current episode is:220000\n",
            "Current episode is:230000\n",
            "Current episode is:240000\n",
            "Current episode is:250000\n",
            "Current episode is:260000\n",
            "Current episode is:270000\n",
            "Current episode is:280000\n",
            "Current episode is:290000\n",
            "Current episode is:300000\n",
            "Current episode is:310000\n",
            "Current episode is:320000\n",
            "Current episode is:330000\n",
            "Current episode is:340000\n",
            "Current episode is:350000\n",
            "Current episode is:360000\n",
            "Current episode is:370000\n",
            "Current episode is:380000\n",
            "Current episode is:390000\n",
            "Current episode is:400000\n",
            "Current episode is:410000\n",
            "Current episode is:420000\n",
            "Current episode is:430000\n",
            "Current episode is:440000\n",
            "Current episode is:450000\n",
            "Current episode is:460000\n",
            "Current episode is:470000\n",
            "Current episode is:480000\n",
            "Current episode is:490000\n",
            "Current episode is:500000\n",
            "Current episode is:510000\n",
            "Current episode is:520000\n",
            "Current episode is:530000\n",
            "Current episode is:540000\n",
            "Current episode is:550000\n",
            "Current episode is:560000\n",
            "Current episode is:570000\n",
            "Current episode is:580000\n",
            "Current episode is:590000\n",
            "Current episode is:600000\n",
            "Current episode is:610000\n",
            "Current episode is:620000\n",
            "Current episode is:630000\n",
            "Current episode is:640000\n",
            "Current episode is:650000\n",
            "Current episode is:660000\n",
            "Current episode is:670000\n",
            "Current episode is:680000\n",
            "Current episode is:690000\n",
            "Current episode is:700000\n",
            "Current episode is:710000\n",
            "Current episode is:720000\n",
            "Current episode is:730000\n",
            "Current episode is:740000\n",
            "Current episode is:750000\n",
            "Current episode is:760000\n",
            "Current episode is:770000\n",
            "Current episode is:780000\n",
            "Current episode is:790000\n",
            "Current episode is:800000\n",
            "Total_time_taken is:  -264.21378207206726\n",
            "Maximum difference 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa_table"
      ],
      "metadata": {
        "id": "dawZNWi9iMrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b6b148-1e26-406b-b07f-8dd273708fad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ -5.        ,  -5.        ,  -5.        ,  -5.        ,\n",
              "         -8.36557069, -48.02693172],\n",
              "       [ -5.        ,  -5.        ,  -5.        ,  -5.        ,\n",
              "         -8.29297897, -47.21837342],\n",
              "       ...,\n",
              "       [ -3.92315857,  -3.9141836 ,  -3.92315857,  -3.91002113,\n",
              "         -8.31261189,  -8.31261189],\n",
              "       [ -4.36316089,  -4.36390835,  -4.36316089,  -4.36767417,\n",
              "         -9.96343246,  -7.46184887],\n",
              "       [ -1.72872094,  -1.75563915,  -1.72872094,  -1.70001111,\n",
              "         -4.80396016,  -6.59372334]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa_table[413]"
      ],
      "metadata": {
        "id": "f18BodMX1Fs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e9a1aa-277f-4084-97c6-20f23c2d21a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -5.        ,  -5.        ,  -5.        ,  -5.        ,\n",
              "       -50.        , -49.99999999])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the SARSA table\n",
        "Let's know test our SARSA agent on a different environment"
      ],
      "metadata": {
        "id": "TWGLeJ52yUsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(\"Current state is:\", state)"
      ],
      "metadata": {
        "id": "LELEyKmT1FvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75ba5cb-a84e-4113-b108-b0b2798e619b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state is: 242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "cumulative_reward = 0\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  best_action = np.argmax(sarsa_table[state,:])\n",
        "  state, reward, done, info = env.step(best_action)\n",
        "  cumulative_reward += reward\n",
        "\n",
        "  time.sleep(0.5)\n",
        "  clear_output(wait = True)\n",
        "  env.render()\n",
        "  print(\"Episode Reward =\", cumulative_reward)"
      ],
      "metadata": {
        "id": "zgHtGacI1FxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f74685-3f9b-4b0f-a801-d94e06aaa1a6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward = -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hU-2c1yQNfTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQDHbBVaNfdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aU2sZE2BNfe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fvm6VFYUNfhS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}